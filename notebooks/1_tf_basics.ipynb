{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![prova](https://upload.wikimedia.org/wikipedia/commons/thumb/a/ab/TensorFlow_logo.svg/1200px-TensorFlow_logo.svg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 17:00:05.172123: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-10 17:00:05.172220: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-10 17:00:05.230504: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-10 17:00:05.338686: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-10 17:00:06.493308: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/andrea/anaconda3/envs/ML-env/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.15.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Manipulation Basics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Creation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 0-dimensional tensor is a __scalar__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of the tensor = 0\n",
      "Number of dimensions = 0\n",
      "Tensor's shape = ()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 17:02:50.472313: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-10 17:02:50.691914: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-10 17:02:50.692213: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-10 17:02:50.694803: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-10 17:02:50.695031: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-10 17:02:50.695226: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-10 17:02:50.771492: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-10 17:02:50.771757: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-10 17:02:50.771997: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-10 17:02:50.772190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2285 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "scalar = tf.constant(0)\n",
    "print(f\"Value of the tensor = {scalar}\")\n",
    "print(f\"Number of dimensions = {len(scalar.shape)}\")\n",
    "print(f\"Tensor's shape = {scalar.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 1-dimensional tensor is a __vector__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of the tensor = [1 2 3]\n",
      "Number of dimensions = 1\n",
      "Tensor's shape = (3,)\n"
     ]
    }
   ],
   "source": [
    "vector = tf.constant([1, 2, 3])\n",
    "print(f\"Value of the tensor = {vector}\")\n",
    "print(f\"Number of dimensions = {len(vector.shape)}\")\n",
    "print(f\"Tensor's shape = {vector.shape}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 2-dimensional tensor is a __matrix__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of the tensor = \n",
      " [[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "Number of dimensions = 2\n",
      "Tensor's shape = (3, 3)\n"
     ]
    }
   ],
   "source": [
    "matrix = tf.constant(\n",
    "    [[1, 2, 3],\n",
    "     [4, 5, 6],\n",
    "     [7, 8, 9]]\n",
    ")\n",
    "print(f\"Value of the tensor = \\n {matrix}\")\n",
    "print(f\"Number of dimensions = {len(matrix.shape)}\")\n",
    "print(f\"Tensor's shape = {matrix.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generalize tensors to __n dimensions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of the tensor = \n",
      " [[[ 0.9989997   1.1218048  -0.33476782]\n",
      "  [ 0.05759341  0.72143805  0.02643689]\n",
      "  [ 0.68869406  0.7444176  -1.1041362 ]]\n",
      "\n",
      " [[-1.5710905  -0.99045753 -0.92024016]\n",
      "  [ 0.0203732   0.9966738   1.1253657 ]\n",
      "  [ 0.69463307 -0.21319962  1.0203186 ]]\n",
      "\n",
      " [[-0.08562597  0.7995403  -0.7123673 ]\n",
      "  [ 1.6473969  -1.8178984  -0.1373985 ]\n",
      "  [-0.16187572  0.23168969 -0.95723665]]]\n",
      "Number of dimensions = 3\n",
      "Tensor's shape = (3, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "n = 3\n",
    "tensor = tf.random.normal(tuple(3 for _ in range(n)))\n",
    "print(f\"Value of the tensor = \\n {tensor}\")\n",
    "print(f\"Number of dimensions = {len(tensor.shape)}\")\n",
    "print(f\"Tensor's shape = {tensor.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each tensor is characterized also by a __data type__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.float32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.dtype"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which can be cast to others (with the clear consequences on the numerical representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0  1  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0 -1]]\n",
      "\n",
      " [[-1  0  0]\n",
      "  [ 0  0  1]\n",
      "  [ 0  0  1]]\n",
      "\n",
      " [[ 0  0  0]\n",
      "  [ 1 -1  0]\n",
      "  [ 0  0  0]]], shape=(3, 3, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "int_tensor = tf.cast(tensor, dtype=tf.int32)\n",
    "print(int_tensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor indexing e slicing\n",
    "\n",
    "Tensors can be __indexed__ (i.e., ``tensor[i, :, :]``) or __sliced__ (i.e., ``tensor[:i, ...]``)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing a tensor reduces its dimensionality depending to the number of \"free\" dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalar = 0.05759340524673462\n",
      "Vector = [-0.33476782 -0.92024016 -0.7123673 ]\n",
      "Matrix = [[ 0.68869406  0.7444176  -1.1041362 ]\n",
      " [ 0.69463307 -0.21319962  1.0203186 ]\n",
      " [-0.16187572  0.23168969 -0.95723665]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Scalar = {tensor[0, 1, 0]}\")\n",
    "print(f\"Vector = {tensor[:, 0, -1]}\") # : means all elements in that dimension\n",
    "print(f\"Matrix = {tensor[:, 2]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slicing reduces the size of the sliced dimension. With this approach, only contiguous slices can be taken. To get scattered slices, use [``tf.gather``](https://www.tensorflow.org/api_docs/python/tf/gather)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_slice = [[[-1.5710905  -0.99045753 -0.92024016]\n",
      "  [ 0.0203732   0.9966738   1.1253657 ]\n",
      "  [ 0.69463307 -0.21319962  1.0203186 ]]\n",
      "\n",
      " [[-0.08562597  0.7995403  -0.7123673 ]\n",
      "  [ 1.6473969  -1.8178984  -0.1373985 ]\n",
      "  [-0.16187572  0.23168969 -0.95723665]]]\n",
      "Shape of tensor_slice = (2, 3, 3)\n",
      "Number of dimensions = 3\n"
     ]
    }
   ],
   "source": [
    "tensor_slice = tensor[1:]\n",
    "print(f\"tensor_slice = {tensor_slice}\")\n",
    "print(f\"Shape of tensor_slice = {tensor_slice.shape}\")\n",
    "print(f\"Number of dimensions = {len(tensor_slice.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_slice = [[[-0.92024016]\n",
      "  [ 1.1253657 ]\n",
      "  [ 1.0203186 ]]\n",
      "\n",
      " [[-0.7123673 ]\n",
      "  [-0.1373985 ]\n",
      "  [-0.95723665]]]\n",
      "Shape of tensor_slice = (2, 3, 1)\n",
      "Number of dimensions = 3\n"
     ]
    }
   ],
   "source": [
    "tensor_slice = tensor[1:, :, -1:]\n",
    "print(f\"tensor_slice = {tensor_slice}\")\n",
    "print(f\"Shape of tensor_slice = {tensor_slice.shape}\")\n",
    "print(f\"Number of dimensions = {len(tensor_slice.shape)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing and slicing can be also mixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_slice = [[-0.92024016  1.1253657   1.0203186 ]\n",
      " [-0.7123673  -0.1373985  -0.95723665]]\n",
      "Shape of tensor_slice = (2, 3)\n",
      "Number of dimensions = 2\n"
     ]
    }
   ],
   "source": [
    "tensor_slice = tensor[1:, :, 2]\n",
    "print(f\"tensor_slice = {tensor_slice}\")\n",
    "print(f\"Shape of tensor_slice = {tensor_slice.shape}\")\n",
    "print(f\"Number of dimensions = {len(tensor_slice.shape)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat and stack\n",
    "\n",
    "- ``tf.concat``: concatenates n tensors on the `axis` dimension. All the dimensions of the input tensors, except for the `axis` dimension, must match.\n",
    "- ``tf.stack``: stacks n tensors on the `axis` dimension, which is added for the resulting tensor. All the dimensions of the input tensors must match. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape = (2, 9, 4)\n"
     ]
    }
   ],
   "source": [
    "tensor_1 = tf.random.normal((2, 3, 4))\n",
    "tensor_2 = tf.random.uniform((2, 6, 4))\n",
    "concat_tensor = tf.concat([tensor_1, tensor_2], axis=1)\n",
    "print(f\"Shape = {concat_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_2 = tf.random.uniform((2, 3, 4))\n",
    "stacked_tensor = tf.stack([tensor_1, tensor_2], axis=1)\n",
    "print(f\"Shape = {stacked_tensor.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn!\n",
    "\n",
    "1. Create a tensor with random numbers of shape (2, 5, 3);\n",
    "2. Get the last element on dimension 1;\n",
    "3. Put it at the beginning of dimension 1.\n",
    "\n",
    "_Note_: use the placeholders on the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_tensor = tf.random.uniform((2,5,3))\n",
    "last = init_tensor[:,-1,:]\n",
    "final_tensor = tf.stack([last, *[init_tensor[:,i,:] for i in range(0,4)]], axis=1)\n",
    "\n",
    "# Mode 2:\n",
    "#last = init_tensor[:,-1:]\n",
    "#final_tensor = tf.concat([last, init_tensor[:,0:-1,:]], axis=1)\n",
    "\n",
    "# Mode 3:\n",
    "#final_tensor = init_tensor[:,[4,0,1,2,3],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good job!\n"
     ]
    }
   ],
   "source": [
    "if final_tensor.shape == (2, 5, 3):\n",
    "    if tf.reduce_all(final_tensor[:, 0] == init_tensor[:, -1]):\n",
    "        print(\"Good job!\")\n",
    "    else:\n",
    "        print(\"Mh, correct dimensions but wrong values\")\n",
    "else:\n",
    "    print(f\"Wrong, dimensions are {init_tensor.shape} and {final_tensor.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor operations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``+, *, /, -`` are overloaded to support tensor operations. All the operations are element-wise, support broadcasting for the non-matching dimensions (i.e., (2, 1, 2) + (2, 3, 2) --> (2, 3, 2))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = tf.random.normal((3, 3, 3))\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_1 = tf.random.normal((3, 2))\n",
    "tensor_2 = tf.random.normal((3, 2))\n",
    "tensor_1 + tensor_2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`@` defines the dot product between two tensors. The inner dimensions must match the criterion for the dot product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_1 @ tf.transpose(tensor_2, [1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_1 = tf.random.normal((10, 3, 2))\n",
    "tensor_2 = tf.random.normal((10, 3, 2))\n",
    "tensor_1 @ tf.transpose(tensor_2, [0, 2, 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning with TensorFlow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digit Classification with a Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "(train_x, train_y), (test_x, test_y) = tf.keras.datasets.mnist.load_data(path='ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label is 5\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APAACzBVBJJwAO9dnp/wm8damu6Dw5dRjGf9IKw/+hkVPffCnWNJa7XVNV0Kxa1hErrNe/M2cnYqgElsAHpjkc1wlAODkV694W8c654t8M6n4TuvEctrrFw0cun3c0/lq+3AMJcDK5AyOeTkd+fPvGFn4gsvEtzF4m89tUG1ZJJjuMgUBVYN/EMKOe9YVXtK0bUtdvVs9LsZ7y4YgbIULYycZPoPc8V6lpfwh0/w7p66z8RdXj0y2z8llC4aWQ+mRn8lz9RXPfE3x1pvi46TYaPZTQadpMJghluWDSyrhQM9SMBe5Oc5NcBV7Tda1XRZJJNK1O8sXkG12tZ2iLD0JUjNQ3l9eahN517dT3MvTfNIXb16n6mq9Ff/2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA90lEQVR4AWNgGMyAWUhIqK5jvdSy/9/rQe5kgTlWjs3KRiAYxHsyKfDzxYMgFiOIAALDvfwQBsO/pK8Mz97fhPLAlNDtvyBwbNv3j8jCUHbAnOy/f89yM2jPwiLJwMc4628UqgQTnPvp/0eGFAQXLg5lcO/764YuhuArf3y4IAfmfoQwlBX44e/fckkMYaiA7q6/f6dJ45IViP3zdzcuSQaGn39/OkBl4WEL4euFmLIwXDuETav6lKfAIPy1DYucRNFdUPCe9MOUE3e6CpI6FogZSEKrwbFyOIATQ5v5mkcgXV9auVGlwK4NDGRguL75b88HVDla8QBFF16ADQA8sQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Label is {train_y[0]}\")\n",
    "Image.fromarray(train_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images need to be flattened to be taken as input by a linear classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = tf.reshape(train_x, [train_x.shape[0], -1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to __rescale__ the values to the interval [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x / 255"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the linear model requires only a weight matrix and a bias vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, b = tf.Variable(tf.random.normal((784, 10))), tf.Variable(tf.zeros(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(60000, 10), dtype=float32, numpy=\n",
       "array([[2.6707292e-05, 3.6736994e-05, 1.5740156e-10, ..., 2.0239747e-03,\n",
       "        9.9753201e-01, 3.2637571e-12],\n",
       "       [2.6779285e-06, 9.5210695e-01, 6.3169634e-12, ..., 1.2047045e-10,\n",
       "        3.9850745e-02, 1.6947183e-13],\n",
       "       [8.9749843e-01, 1.3932763e-06, 2.8866756e-05, ..., 1.4765062e-06,\n",
       "        4.9212256e-05, 1.1547863e-05],\n",
       "       ...,\n",
       "       [2.8467437e-06, 4.3535153e-09, 6.0912813e-16, ..., 1.4093153e-12,\n",
       "        9.9999714e-01, 6.1568529e-17],\n",
       "       [6.3736372e-09, 1.2762776e-01, 9.1636108e-12, ..., 6.8444863e-02,\n",
       "        7.9749018e-01, 8.2558044e-11],\n",
       "       [1.8420640e-02, 3.2786937e-07, 1.3132326e-07, ..., 2.5935289e-09,\n",
       "        9.7394222e-01, 9.4692354e-07]], dtype=float32)>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = tf.nn.softmax(train_x @ W + b, axis=-1)\n",
    "prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our linear model, let's define the tools for the optimization, i.e., __the optimizer and the loss function__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-1)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A training loop with TensorFlow is shaped as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 17:33:48.175758: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: accuracy = 0.08216666666666667\n",
      "Epoch 20: accuracy = 0.12561666666666665\n",
      "Epoch 40: accuracy = 0.20545\n",
      "Epoch 60: accuracy = 0.27381666666666665\n",
      "Epoch 80: accuracy = 0.33375\n",
      "Epoch 100: accuracy = 0.3839166666666667\n",
      "Epoch 120: accuracy = 0.4248\n",
      "Epoch 140: accuracy = 0.45818333333333333\n",
      "Epoch 160: accuracy = 0.4865\n",
      "Epoch 180: accuracy = 0.5105\n",
      "Epoch 200: accuracy = 0.52995\n",
      "Epoch 220: accuracy = 0.5475166666666667\n",
      "Epoch 240: accuracy = 0.5626333333333333\n",
      "Epoch 260: accuracy = 0.57685\n",
      "Epoch 280: accuracy = 0.5878833333333333\n",
      "Epoch 300: accuracy = 0.5991166666666666\n",
      "Epoch 320: accuracy = 0.6098333333333333\n",
      "Epoch 340: accuracy = 0.6192\n",
      "Epoch 360: accuracy = 0.6289666666666667\n",
      "Epoch 380: accuracy = 0.6422333333333333\n",
      "Epoch 400: accuracy = 0.6573833333333333\n",
      "Epoch 420: accuracy = 0.6718\n",
      "Epoch 440: accuracy = 0.6847166666666666\n",
      "Epoch 460: accuracy = 0.6958166666666666\n",
      "Epoch 480: accuracy = 0.7055166666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "epochs = 500 \n",
    "\n",
    "for e in range(epochs):\n",
    "    # The GradientTape context records every operation applied to tensors\n",
    "    # inside the context. The tape can then be used to compute the gradient\n",
    "    # of a computation with respect to the tensors it has \"watched\".\n",
    "    with tf.GradientTape() as tape:\n",
    "        prediction = tf.nn.softmax(train_x @ W + b, axis=-1)\n",
    "        loss_value = loss_fn(train_y, prediction)\n",
    "    \n",
    "    # We compute the gradient of the loss with respect to the parameters\n",
    "    # of the model\n",
    "    grads = tape.gradient(loss_value, [W, b])\n",
    "\n",
    "    # We apply the gradient to the parameters of the model\n",
    "    optimizer.apply_gradients(zip(grads, [W, b]))\n",
    "\n",
    "    # We print the loss every 20 epochs\n",
    "    prediction = tf.nn.softmax(train_x @ W + b, axis=-1)\n",
    "    if e % 20 == 0:\n",
    "        print(f\"Epoch {e}: accuracy = {accuracy_score(train_y, tf.argmax(prediction, axis=-1))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, if we use Adam as optimization approach, the performance in the learning phase increases dramatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca 0: accuratezza = 0.12605\n",
      "Epoca 20: accuratezza = 0.8735666666666667\n",
      "Epoca 40: accuratezza = 0.9051\n",
      "Epoca 60: accuratezza = 0.9150666666666667\n",
      "Epoca 80: accuratezza = 0.9205\n",
      "Epoca 100: accuratezza = 0.9235833333333333\n",
      "Epoca 120: accuratezza = 0.9262833333333333\n",
      "Epoca 140: accuratezza = 0.9288\n",
      "Epoca 160: accuratezza = 0.9310666666666667\n",
      "Epoca 180: accuratezza = 0.9326\n",
      "Epoca 200: accuratezza = 0.9336833333333333\n",
      "Epoca 220: accuratezza = 0.8906666666666667\n",
      "Epoca 240: accuratezza = 0.9332\n",
      "Epoca 260: accuratezza = 0.93745\n",
      "Epoca 280: accuratezza = 0.9380333333333334\n",
      "Epoca 300: accuratezza = 0.9388166666666666\n",
      "Epoca 320: accuratezza = 0.93935\n",
      "Epoca 340: accuratezza = 0.9399333333333333\n",
      "Epoca 360: accuratezza = 0.9329166666666666\n",
      "Epoca 380: accuratezza = 0.9352333333333334\n",
      "Epoca 400: accuratezza = 0.9397\n",
      "Epoca 420: accuratezza = 0.9419333333333333\n",
      "Epoca 440: accuratezza = 0.94195\n",
      "Epoca 460: accuratezza = 0.9422666666666667\n",
      "Epoca 480: accuratezza = 0.9424666666666667\n"
     ]
    }
   ],
   "source": [
    "W, b = tf.Variable(tf.random.normal((784, 10))), tf.Variable(tf.zeros(10))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "epochs = 500\n",
    "\n",
    "for e in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        prediction = tf.nn.softmax(train_x @ W + b, axis=-1)\n",
    "        loss_value = loss_fn(train_y, prediction)\n",
    "    \n",
    "    grads = tape.gradient(loss_value, [W, b])\n",
    "    optimizer.apply_gradients(zip(grads, [W, b]))\n",
    "\n",
    "    if e % 20 == 0:\n",
    "        prediction = tf.nn.softmax(train_x @ W + b, axis=-1)\n",
    "        print(f\"Epoca {e}: accuratezza = {accuracy_score(train_y, tf.argmax(prediction, axis=-1))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to implement your own training loop with a linear regression on the Boston Housing dataset. \n",
    "\n",
    "Notes:\n",
    "1. It is a regression problem, so you need to use a different loss function;\n",
    "2. MinMax Scaling may not be the most appropriate way to rescale the input features (maybe sklearn's StandardScaler?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A local file was found, but it seems to be incomplete or outdated because the auto file hash does not match the original value of f553886a1f8d56431e820c5b82552d9d95cfcb96d1e678153f8839538947dff5 so we will re-download the data.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
      "\u001b[1m57026/57026\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4us/step\n"
     ]
    }
   ],
   "source": [
    "(train_x, train_y), _ = tf.keras.datasets.boston_housing.load_data(\"./ds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((404, 13), (404,))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13,), 15.2)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[0].shape, train_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling (later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddenLayer = 1\n",
    "W, b = tf.Variable(tf.random.normal((13,hiddenLayer))), tf.Variable(tf.zeros(hiddenLayer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-1)\n",
    "loss = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Argument `output` must have rank (ndim) `target.ndim - 1`. Received: target.shape=(404,), output.shape=(404,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m      9\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m train_x \u001b[38;5;241m@\u001b[39m W \u001b[38;5;241m+\u001b[39m b\n\u001b[0;32m---> 10\u001b[0m     loss_value \u001b[38;5;241m=\u001b[39m loss_fn(train_y, prediction)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# We compute the gradient of the loss with respect to the parameters\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# of the model\u001b[39;00m\n\u001b[1;32m     14\u001b[0m grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss_value, [W, b])\n",
      "File \u001b[0;32m~/anaconda3/envs/ML-env/lib/python3.11/site-packages/keras/src/losses/loss.py:43\u001b[0m, in \u001b[0;36mLoss.__call__\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m     36\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype), y_pred\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     39\u001b[0m y_true \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype), y_true\n\u001b[1;32m     41\u001b[0m )\n\u001b[0;32m---> 43\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall(y_true, y_pred)\n\u001b[1;32m     44\u001b[0m out_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(losses, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m in_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m out_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/ML-env/lib/python3.11/site-packages/keras/src/losses/losses.py:22\u001b[0m, in \u001b[0;36mLossFunctionWrapper.call\u001b[0;34m(self, y_true, y_pred)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, y_true, y_pred):\n\u001b[1;32m     21\u001b[0m     y_true, y_pred \u001b[38;5;241m=\u001b[39m squeeze_to_same_rank(y_true, y_pred)\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(y_true, y_pred, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fn_kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ML-env/lib/python3.11/site-packages/keras/src/losses/losses.py:1714\u001b[0m, in \u001b[0;36msparse_categorical_crossentropy\u001b[0;34m(y_true, y_pred, from_logits, ignore_class, axis)\u001b[0m\n\u001b[1;32m   1709\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m y_true \u001b[38;5;241m*\u001b[39m ops\u001b[38;5;241m.\u001b[39mcast(valid_mask, y_true\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m   1710\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m y_pred \u001b[38;5;241m*\u001b[39m ops\u001b[38;5;241m.\u001b[39mcast(\n\u001b[1;32m   1711\u001b[0m         ops\u001b[38;5;241m.\u001b[39mexpand_dims(valid_mask, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), y_pred\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m   1712\u001b[0m     )\n\u001b[0;32m-> 1714\u001b[0m res \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39msparse_categorical_crossentropy(\n\u001b[1;32m   1715\u001b[0m     y_true,\n\u001b[1;32m   1716\u001b[0m     y_pred,\n\u001b[1;32m   1717\u001b[0m     from_logits\u001b[38;5;241m=\u001b[39mfrom_logits,\n\u001b[1;32m   1718\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   1719\u001b[0m )\n\u001b[1;32m   1721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1722\u001b[0m     valid_mask \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mreshape(valid_mask, res_shape)\n",
      "File \u001b[0;32m~/anaconda3/envs/ML-env/lib/python3.11/site-packages/keras/src/ops/nn.py:1541\u001b[0m, in \u001b[0;36msparse_categorical_crossentropy\u001b[0;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((target, output)):\n\u001b[1;32m   1538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparseCategoricalCrossentropy(\n\u001b[1;32m   1539\u001b[0m         from_logits\u001b[38;5;241m=\u001b[39mfrom_logits, axis\u001b[38;5;241m=\u001b[39maxis\n\u001b[1;32m   1540\u001b[0m     )\u001b[38;5;241m.\u001b[39msymbolic_call(target, output)\n\u001b[0;32m-> 1541\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msparse_categorical_crossentropy(\n\u001b[1;32m   1542\u001b[0m     target, output, from_logits\u001b[38;5;241m=\u001b[39mfrom_logits, axis\u001b[38;5;241m=\u001b[39maxis\n\u001b[1;32m   1543\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/ML-env/lib/python3.11/site-packages/keras/src/backend/tensorflow/nn.py:616\u001b[0m, in \u001b[0;36msparse_categorical_crossentropy\u001b[0;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    611\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `output` must be at least rank 1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    612\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    613\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    614\u001b[0m     )\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m--> 616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    617\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `output` must have rank (ndim) `target.ndim - 1`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    618\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    619\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    620\u001b[0m     )\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e1, e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape, output\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e1 \u001b[38;5;241m!=\u001b[39m e2:\n",
      "\u001b[0;31mValueError\u001b[0m: Argument `output` must have rank (ndim) `target.ndim - 1`. Received: target.shape=(404,), output.shape=(404,)"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "epochs = 420\n",
    "\n",
    "for e in range(epochs):\n",
    "    # The GradientTape context records every operation applied to tensors\n",
    "    # inside the context. The tape can then be used to compute the gradient\n",
    "    # of a computation with respect to the tensors it has \"watched\".\n",
    "    with tf.GradientTape() as tape:\n",
    "        prediction = train_x @ W + b\n",
    "        loss_value = loss_fn(train_y, prediction)\n",
    "    \n",
    "    # We compute the gradient of the loss with respect to the parameters\n",
    "    # of the model\n",
    "    grads = tape.gradient(loss_value, [W, b])\n",
    "\n",
    "    # We apply the gradient to the parameters of the model\n",
    "    optimizer.apply_gradients(zip(grads, [W, b]))\n",
    "\n",
    "    # We print the loss every 20 epochs\n",
    "    prediction = train_x @ W + b\n",
    "    if e % 20 == 0:\n",
    "        #print('ciao')\n",
    "        print(f\"Epoch {e}: mse = {mean_squared_error(train_y, prediction)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tf_mela')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3917c018cfd92834e25e8ac39593c8ba50cba5d103d91e5ca7d5a867c45ae1af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
